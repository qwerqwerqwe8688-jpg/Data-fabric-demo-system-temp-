# backend/services/lineage_discovery.py
"""
æ–¹æ¡ˆ2å®Œæ•´å®ç°ï¼šè‡ªåŠ¨å‘ç°æ—¶åŒæ—¶å†™ LINEAGE è¾¹ï¼ˆä¸ DERIVED_FROM åŒæ„ï¼‰
"""
import json
import os
import re
import hashlib
from difflib import SequenceMatcher
from typing import Dict, List, Tuple, Set
from pathlib import Path

import pandas as pd
import sqlglot

from backend.services.graph_service import GraphService


# ------------------------------------------------------------------
# å·¥å…·å‡½æ•°
# ------------------------------------------------------------------
def _safe_id(raw: str) -> str:
    return re.sub(r'[^0-9A-Za-z._-]', '_', str(raw))


def _is_similar_name(n1: str, n2: str, threshold: float = 0.7) -> bool:
    n1_clean = n1.lower().replace('_col', '').replace('_', '')
    n2_clean = n2.lower().replace('_col', '').replace('_', '')
    return SequenceMatcher(None, n1_clean, n2_clean).ratio() >= threshold


def _column_fingerprint(series: pd.Series, sample: int = 100) -> str:
    valid = series.dropna().astype(str)
    if len(valid) == 0:
        return ""
    sampled = sorted(valid.sample(min(sample, len(valid))))
    return hashlib.sha256("".join(sampled).encode("utf-8")).hexdigest()


def _is_pk_candidate(series: pd.Series, unique_ratio: float = 0.8) -> bool:
    if len(series.dropna()) == 0:
        return False
    return series.nunique() / len(series.dropna()) >= unique_ratio


def _is_fk_candidate(series: pd.Series, pk_series: pd.Series) -> bool:
    fk_vals = set(series.dropna().astype(str))
    pk_vals = set(pk_series.dropna().astype(str))
    if not fk_vals:
        return False
    common_vals = fk_vals.intersection(pk_vals)
    return len(common_vals) > 0 and len(common_vals) / len(fk_vals) >= 0.1


# ------------------------------------------------------------------
# SQL è§£æ
# ------------------------------------------------------------------
def parse_sql_column_lineage(sql_path: Path) -> Dict[str, Set[str]]:
    lineage: Dict[str, Set[str]] = {}
    try:
        with open(sql_path, encoding="utf-8-sig") as f:
            sql_content = f.read()

        target_match = re.search(r"--\s*target:\s*(\w+)", sql_content, re.I)
        if not target_match:
            return lineage
        target_table = target_match.group(1)

        try:
            parsed = sqlglot.parse(sql_content)
            if not parsed:
                return lineage
        except Exception:
            return _fallback_sql_parsing(sql_content, target_table)

        for stmt in parsed:
            for select in stmt.find_all(sqlglot.expressions.Select):
                for expr in select.expressions:
                    src_tables, src_columns = set(), set()
                    for column in expr.find_all(sqlglot.expressions.Column):
                        if column.table:
                            src_tables.add(column.table)
                        if column.name:
                            src_columns.add(column.name)
                    target_col = expr.alias_or_name if hasattr(expr, 'alias_or_name') else "unknown"

                    for src_table in src_tables:
                        for src_col in src_columns:
                            src_full = f"file.{_safe_id(src_table)}.{_safe_id(src_col)}"
                            tgt_full = f"virtual.{_safe_id(target_table)}.{_safe_id(target_col)}"
                            lineage.setdefault(tgt_full, set()).add(src_full)
        return lineage

    except Exception as e:
        print(f"âŒ SQL è§£æå¤±è´¥ {sql_path}: {e}")
        return lineage


def _fallback_sql_parsing(sql_content: str, target_table: str) -> Dict[str, Set[str]]:
    lineage = {}
    insert_pattern = r'INSERT\s+INTO\s+(\w+)\s+SELECT\s+(.+?)\s+FROM\s+(\w+)'
    for m in re.finditer(insert_pattern, sql_content, re.I | re.S):
        tgt, cols, src = m.groups()
        for i, col in enumerate(cols.split(',')):
            src_col = f"col_{i}"
            src_full = f"file.{_safe_id(src)}.{_safe_id(src_col)}"
            tgt_full = f"virtual.{_safe_id(tgt)}.{_safe_id(col.strip())}"
            lineage.setdefault(tgt_full, set()).add(src_full)

    view_pattern = r'CREATE\s+VIEW\s+(\w+)\s+AS\s+SELECT\s+(.+?)\s+FROM\s+(\w+)'
    for m in re.finditer(view_pattern, sql_content, re.I | re.S):
        tgt, cols, src = m.groups()
        for col_expr in cols.split(','):
            col_match = re.match(r'(.+?)\s+AS\s+(\w+)', col_expr, re.I)
            if col_match:
                src_col = col_match.group(1).strip().split()[-1]
                tgt_col = col_match.group(2).strip()
                src_full = f"file.{_safe_id(src)}.{_safe_id(src_col)}"
                tgt_full = f"virtual.{_safe_id(tgt)}.{_safe_id(tgt_col)}"
                lineage.setdefault(tgt_full, set()).add(src_full)
    return lineage


class AutoLineageService:
    def __init__(self, graph_service: GraphService):
        self.gs = graph_service

    # æ–°å¢ç¼ºå¤±çš„æ–¹æ³•
    def discover_by_name(self):
        """åŸºäºåç§°ç›¸ä¼¼æ€§å‘ç°è¡€ç¼˜å…³ç³»"""
        print("ğŸ” å¼€å§‹åŸºäºåç§°ç›¸ä¼¼æ€§çš„è¡€ç¼˜å‘ç°...")
        try:
            with self.gs.driver.session() as session:
                # æŸ¥æ‰¾åç§°ç›¸ä¼¼çš„åˆ—
                result = session.run("""
                    MATCH (c1:Column), (c2:Column)
                    WHERE c1.id < c2.id 
                    AND c1.name =~ '(?i).*' + replace(c2.name, '_', '.*') + '.*'
                    AND c1.id STARTS WITH 'file.' AND c2.id STARTS WITH 'file.'
                    MERGE (c1)-[:DERIVED_FROM {method: 'name_similarity', level: 'column'}]->(c2)
                    MERGE (c1)-[:LINEAGE {type: 'DERIVED_FROM', level: 'column'}]->(c2)
                    RETURN count(*) as relationships_created
                """)
                count = result.single()["relationships_created"]
                print(f"âœ… åŸºäºåç§°ç›¸ä¼¼æ€§å‘ç° {count} ä¸ªè¡€ç¼˜å…³ç³»")
        except Exception as e:
            print(f"âŒ åŸºäºåç§°ç›¸ä¼¼æ€§çš„è¡€ç¼˜å‘ç°å¤±è´¥: {e}")

    def discover_by_fk(self, csv_root: Path):
        """åŸºäºå¤–é”®å…³ç³»å‘ç°è¡€ç¼˜å…³ç³»"""
        print("ğŸ” å¼€å§‹åŸºäºå¤–é”®å…³ç³»çš„è¡€ç¼˜å‘ç°...")
        try:
            # è¿™é‡Œå¯ä»¥æ·»åŠ å…·ä½“çš„å¤–é”®å‘ç°é€»è¾‘
            # ä¾‹å¦‚åˆ†æCSVæ–‡ä»¶ä¸­çš„æ•°æ®å…³ç³»
            count = 0
            print(f"âœ… åŸºäºå¤–é”®å…³ç³»å‘ç° {count} ä¸ªè¡€ç¼˜å…³ç³»")
        except Exception as e:
            print(f"âŒ åŸºäºå¤–é”®å…³ç³»çš„è¡€ç¼˜å‘ç°å¤±è´¥: {e}")

    def discover_by_fingerprint(self, csv_root: Path):
        """åŸºäºæ•°æ®æŒ‡çº¹å‘ç°è¡€ç¼˜å…³ç³»"""
        print("ğŸ” å¼€å§‹åŸºäºæ•°æ®æŒ‡çº¹çš„è¡€ç¼˜å‘ç°...")
        try:
            # è¿™é‡Œå¯ä»¥æ·»åŠ æ•°æ®æŒ‡çº¹åˆ†æé€»è¾‘
            count = 0
            print(f"âœ… åŸºäºæ•°æ®æŒ‡çº¹å‘ç° {count} ä¸ªè¡€ç¼˜å…³ç³»")
        except Exception as e:
            print(f"âŒ åŸºäºæ•°æ®æŒ‡çº¹çš„è¡€ç¼˜å‘ç°å¤±è´¥: {e}")

    def discover_by_sql(self, sql_dir: Path):
        """åŸºäºSQLè§£æå‘ç°è¡€ç¼˜å…³ç³»"""
        print("ğŸ” å¼€å§‹åŸºäºSQLè§£æçš„è¡€ç¼˜å‘ç°...")
        try:
            if not sql_dir.exists():
                print(f"âŒ SQLç›®å½•ä¸å­˜åœ¨: {sql_dir}")
                return

            sql_files = list(sql_dir.glob("*.sql"))
            if not sql_files:
                print("â„¹ï¸ æœªæ‰¾åˆ°SQLæ–‡ä»¶")
                return

            relationships_created = 0
            for sql_file in sql_files:
                try:
                    lineage = parse_sql_column_lineage(sql_file)
                    with self.gs.driver.session() as session:
                        for target, sources in lineage.items():
                            for source in sources:
                                session.run("""
                                    MERGE (src:DataAsset {id: $source_id})
                                    MERGE (tgt:DataAsset {id: $target_id})
                                    MERGE (src)-[:DERIVED_FROM {method: 'sql_parsing', level: 'column'}]->(tgt)
                                    MERGE (src)-[:LINEAGE {type: 'DERIVED_FROM', level: 'column'}]->(tgt)
                                """, source_id=source, target_id=target)
                                relationships_created += 1
                except Exception as e:
                    print(f"âŒ å¤„ç†SQLæ–‡ä»¶ {sql_file} å¤±è´¥: {e}")

            print(f"âœ… åŸºäºSQLè§£æå‘ç° {relationships_created} ä¸ªè¡€ç¼˜å…³ç³»ï¼Œå¤„ç†äº† {len(sql_files)} ä¸ªSQLæ–‡ä»¶")
        except Exception as e:
            print(f"âŒ åŸºäºSQLè§£æçš„è¡€ç¼˜å‘ç°å¤±è´¥: {e}")

    # æ–°å¢ï¼šè¡Œçº§æ•°æ®ç›¸ä¼¼æ€§åˆ†æ
    def discover_row_similarity(self, csv_root: Path, similarity_threshold: float = 0.8):
        """åŸºäºè¡Œæ•°æ®ç›¸ä¼¼æ€§å‘ç°è¡Œçº§è¡€ç¼˜å…³ç³»"""
        print("ğŸ” å¼€å§‹è¡Œçº§æ•°æ®ç›¸ä¼¼æ€§åˆ†æ...")

        all_rows = {}
        with self.gs.driver.session() as session:
            # æ”¶é›†æ‰€æœ‰è¡Œæ•°æ®
            result = session.run("""
                MATCH (r:DataAsset {type: 'row'})
                WHERE r.id STARTS WITH 'file.' AND r.row_data IS NOT NULL
                RETURN r.id as row_id, r.row_data as row_data, r.table_id as table_id
            """)

            for record in result:
                row_data = record["row_data"]
                if isinstance(row_data, str):
                    try:
                        row_data = json.loads(row_data)
                    except:
                        continue

                all_rows[record["row_id"]] = {
                    "data": row_data,
                    "table": record["table_id"]
                }

        # è®¡ç®—è¡Œé—´ç›¸ä¼¼åº¦
        matches = 0
        row_ids = list(all_rows.keys())

        with self.gs.driver.session() as session:
            for i in range(len(row_ids)):
                for j in range(i + 1, len(row_ids)):
                    row1_id = row_ids[i]
                    row2_id = row_ids[j]

                    row1_data = all_rows[row1_id]["data"]
                    row2_data = all_rows[row2_id]["data"]

                    similarity = self._calculate_row_similarity(row1_data, row2_data)

                    if similarity >= similarity_threshold:
                        # åˆ›å»ºè¡Œçº§è¡€ç¼˜å…³ç³»
                        session.run("""
                            MATCH (r1:DataAsset {id: $row1_id}), (r2:DataAsset {id: $row2_id})
                            MERGE (r1)-[:DERIVED_FROM {
                                method: 'row_similarity', 
                                similarity: $similarity,
                                level: 'row'
                            }]->(r2)
                            MERGE (r1)-[:LINEAGE {
                                type: 'DERIVED_FROM',
                                level: 'row',
                                similarity: $similarity
                            }]->(r2)
                        """, row1_id=row1_id, row2_id=row2_id, similarity=similarity)
                        matches += 1

        print(f"âœ… è¡Œçº§ç›¸ä¼¼æ€§åˆ†æå®Œæˆï¼Œå‘ç° {matches} ä¸ªè¡Œçº§åŒ¹é…")

    def _calculate_row_similarity(self, row1: Dict, row2: Dict) -> float:
        """è®¡ç®—ä¸¤è¡Œæ•°æ®çš„ç›¸ä¼¼åº¦"""
        if not row1 or not row2:
            return 0.0

        common_keys = set(row1.keys()) & set(row2.keys())
        if not common_keys:
            return 0.0

        matches = 0
        for key in common_keys:
            if row1.get(key) == row2.get(key):
                matches += 1

        return matches / len(common_keys)

    # æ–°å¢ï¼šä¿ƒé”€æ•°æ®åˆ†æç‰¹å®šçš„è¡€ç¼˜å‘ç°
    def discover_promotion_lineage(self, csv_root: Path):
        """é’ˆå¯¹ä¿ƒé”€æ•°æ®çš„ä¸“ä¸šè¡€ç¼˜åˆ†æ"""
        print("ğŸ” å¼€å§‹ä¿ƒé”€æ•°æ®ä¸“ä¸šè¡€ç¼˜åˆ†æ...")

        promotion_patterns = {
            "discount_derivation": self._analyze_discount_derivation,
            "budget_allocation": self._analyze_budget_allocation,
            "performance_correlation": self._analyze_performance_correlation
        }

        for pattern_name, analysis_func in promotion_patterns.items():
            try:
                count = analysis_func(csv_root)
                print(f"  âœ… {pattern_name}: å‘ç° {count} ä¸ªå…³ç³»")
            except Exception as e:
                print(f"  âŒ {pattern_name} åˆ†æå¤±è´¥: {e}")

    def _analyze_discount_derivation(self, csv_root: Path) -> int:
        """åˆ†ææŠ˜æ‰£ç‡æ¨å¯¼å…³ç³»"""
        relationships = 0
        with self.gs.driver.session() as session:
            # æŸ¥æ‰¾åŒ…å«ä»·æ ¼å’ŒæŠ˜æ‰£çš„åˆ—
            result = session.run("""
                MATCH (c:Column) 
                WHERE c.name =~ '(?i).*æŠ˜æ‰£.*|.*discount.*|.*ç‡.*'
                RETURN c.id as col_id, c.name as col_name
            """)

            for record in result:
                col_id = record["col_id"]
                # è¿™é‡Œå¯ä»¥æ·»åŠ å…·ä½“çš„æŠ˜æ‰£æ¨å¯¼é€»è¾‘
                # ä¾‹å¦‚ï¼šæŠ˜æ‰£ç‡ = (åŸä»·-ä¿ƒé”€ä»·)/åŸä»·
                pass

        return relationships

    def _analyze_budget_allocation(self, csv_root: Path) -> int:
        """åˆ†æé¢„ç®—åˆ†é…å…³ç³»"""
        # å®ç°é¢„ç®—åˆ†é…åˆ†æé€»è¾‘
        return 0

    def _analyze_performance_correlation(self, csv_root: Path) -> int:
        """åˆ†æä¸šç»©ç›¸å…³æ€§"""
        # å®ç°ä¸šç»©ç›¸å…³æ€§åˆ†æé€»è¾‘
        return 0

    # ä¿®æ”¹discover_allæ–¹æ³•ï¼ŒåŠ å…¥æ‰€æœ‰å‘ç°æ–¹æ³•
    def discover_all(self, csv_root: Path, sql_dir: Path):
        print("ğŸš€ å¼€å§‹å…¨é¢è¡€ç¼˜å‘ç°...")

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        csv_root.mkdir(exist_ok=True)
        sql_dir.mkdir(exist_ok=True)

        # æ‰§è¡Œæ‰€æœ‰è¡€ç¼˜å‘ç°æ–¹æ³•
        self.discover_by_name()
        self.discover_by_fk(csv_root)
        self.discover_by_fingerprint(csv_root)
        self.discover_by_sql(sql_dir)

        # æ–°å¢è¡Œçº§åˆ†æ
        self.discover_row_similarity(csv_root)
        self.discover_promotion_lineage(csv_root)

        print("ğŸ‰ å…¨é¢è¡€ç¼˜å‘ç°å®Œæˆï¼ˆåŒ…å«è¡Œçº§åˆ†æï¼‰")


# ------------------------------------------------------------------
# å‘åå…¼å®¹å¯¼å‡º
# ------------------------------------------------------------------
def discover_sql_lineage_in_directory(sql_dir: str, graph_service: GraphService):
    AutoLineageService(graph_service).discover_by_sql(Path(sql_dir))
    return {"sql_files_found": len(list(Path(sql_dir).glob("*.sql"))), "successful_discoveries": True}


def discover_lineage_auto(graph_service: GraphService, csv_root: str, sql_dir: str):
    AutoLineageService(graph_service).discover_all(Path(csv_root), Path(sql_dir))


def get_lineage_graph_for_frontend() -> list:
    gs = GraphService("bolt://localhost:7687", "neo4j", "password")
    with gs.driver.session() as session:
        nodes = session.run("""
            MATCH (c:Column)
            RETURN c.id as id, c.name as label, 'column' as group
            LIMIT 100
        """).data()
        edges = session.run("""
            MATCH (src:Column)-[r:LINEAGE]->(dst:Column)
            RETURN src.id as from, dst.id as to, r.type as method
            LIMIT 200
        """).data()
    gs.close()

    result = []
    for node in nodes:
        result.append({"id": node["id"], "label": node["label"], "group": node["group"], "type": "node"})
    for edge in edges:
        result.append({"from": edge["from"], "to": edge["to"], "method": edge.get("method", "unknown"), "type": "edge"})
    return result